{
  "metadata": {
    "name": "GraphX Exercise Initial",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "sc\nimport org.apache.log4j.{Level, Logger}\nimport org.apache.spark.graphx._\nimport org.apache.spark.storage.StorageLevel\nimport org.apache.spark.{SparkConf, SparkContext}\nimport java.io.{File, PrintWriter}\n\nimport scala.util.Random\n\nval storageLevel \u003d StorageLevel.MEMORY_AND_DISK_SER\n\nval awspath \u003d \"s3n://alssn-bucket/\"\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val edges \u003d sc.textFile(awspath+\"CA-HepPh.txt\",200)\nprintln(edges.partitions.size)\nedges.count"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "edges.take(50).foreach(println)\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val edges \u003d sc.textFile(awspath+\"CA-HepPh.txt\",200).filter{\n    case(line:String)\u003d\u003e\n    !line.startsWith(\"#\")\n}.map{\n    case(line:String)\u003d\u003e\n    val lineArray\u003d line.split(\"\\t\").filter(_.length()\u003e0)\n    Edge(lineArray(0).toLong,lineArray(1).toLong,0L)\n}\nprintln(edges.count)"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val graph \u003d Graph.fromEdges(edges,1L).cache()\ngraph.vertices.count"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val dg \u003d graph.degrees\n//degree.sortByKey().take(10).foreach(println)\nval dd \u003d dg.map{case(id,d)\u003d\u003ed-\u003e1}.reduceByKey(_+_)\ndd.sortByKey().take(10).foreach(println)"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "println(\"%table\\ndegree\\tcount\\n\")\ndd.collect.foreach{case(id:Int,d:Int)\u003d\u003eprintln(id+\"\\t\"+d)}"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val cc \u003d graph.connectedComponents().vertices\ncc.take(10).foreach(println)"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val cc_count \u003d cc.map{case(id,cc)\u003d\u003ecc-\u003e1}.reduceByKey(_+_).sortByKey()\nprintln(cc_count.count)\ncc_count.take(20).foreach(println)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.graphx.lib._\nval sp \u003d ShortestPaths.run(graph,List(1L))\nsp.vertices.filter{case(id,m)\u003d\u003em.size\u003e0}.take(10).foreach(println)"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "//val lp \u003d LabelPropagation.run(graph,5).cache()\n//lp.vertices.take(10).foreach(println)\nlp.vertices.filter{case(id,m)\u003d\u003em\u003d\u003d19437}.take(10).foreach(println)\nlp.vertices.filter{case(id,m)\u003d\u003eid\u003d\u003d19437}.take(10).foreach(println)\ncc.filter{case(id,m)\u003d\u003eid\u003d\u003d19437}.collect.foreach(println)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import scala.reflect.ClassTag\nimport org.apache.spark.graphx._\n\nobject degree_new {\n\n  def run[VD: ClassTag, ED: ClassTag](graph: Graph[VD, ED]): Graph[VertexId, ED] \u003d {\n    require(maxIterations \u003e 0, s\"Maximum of iterations must be greater than 0,\" +\n      s\" but got ${maxIterations}\")\n\n    val maxIterations\u003d ???\n    \n    val degGraph \u003d graph.mapVertices { case (vid, _) \u003d\u003e vid }\n    \n    def sendMessage(edge: EdgeTriplet[VertexId, ED]): Iterator[(VertexId, VertexId)] \u003d {\n     ???\n    }\n    \n    val initialMessage \u003d ???\n    \n    val pregelGraph \u003d Pregel(ccGraph, initialMessage,\n      maxIterations, EdgeDirection.??? )(\n      vprog \u003d (id, attr, msg) \u003d\u003e math.min(attr, msg),\n      sendMsg \u003d sendMessage,\n      mergeMsg \u003d (a, b) \u003d\u003e math.min(a, b))\n    ccGraph.unpersist()\n    pregelGraph\n  } // end of degree\n\n}\n\nval degn \u003d degree_new.run(graph).vertices\ndegn.take(10).foreach(println)"
    }
  ]
}